# Mechanistic Interpretability of Transformers Models

## Limitation of GPT-2 in Mathematical Comparisons

### Assignmemt for NLP Master's Course of AI of University of Alicante

#### by Gabriel Masella and Laihi Bahar Eddine

------

The objective of this assignment is to explore Mechanistic Interpretability in Transformer models
through the implementation of activation patching on a GPT-2 model. The task involves the execution
of GPT-2 with two inputs, consisting of a clean text and a corrupted text that differs by a single token.
The model output probabilities will then be analysed to identify the changes caused by the corruption.
By intervening in specific model activations and comparing the probabilities, it is possible to gain
insights into how individual activations influence the modelâ€™s predictions.
In particular, in this project the objective is to investigate the activation patching process in two
scenarios: firstly, by demonstrating the correct propagation of information in a basic example; and
secondly, by testing the model with a more complex example involving a mathematical comparison.

The aim of this assignment is to explore mechanistic interpretability in transformer models by implementing activation patching on a GPT2 model. The task involves running GPT2 with two inputs, a clean text and a corrupted one, differing by a single token and analyzing the change in model output probabilities. By intervening in specific model activations and comparing the probabilities, it possible to gain insights into how individual activations influence the model's predictions.

If you want to know more about the implementation and the results obtained, you can read everything in the [**Final Report**](https://github.com/Gabrynho/minGPT_NLP/NLPT_Final_Report_GM_BEL.pdf).
